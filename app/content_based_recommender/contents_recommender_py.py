# í‚¤ì›Œë“œ ì¶”ì²œ ê°œì„ 
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import re
import os, pickle, json
from collections import Counter
from konlpy.tag import Okt

class ImprovedMovieRecommendationSystem:
    def __init__(self, stopword_files=None, cache_dir_name="cached_features"):
        self.kobert_tokenizer = None
        self.kobert_model = None
        self.tfidf_title = None
        self.tfidf_overview = None
        self.tfidf_keywords = None
        self.mlb_genre = None
        self.movie_data = None
        self.plot_embeddings = None
        self.title_matrix = None
        self.genre_matrix = None
        self.keyword_matrix = None
        self.overview_matrix = None

        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.cache_dir = os.path.join(self.script_dir, cache_dir_name)

        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
            print(f"ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {self.cache_dir}")

        self.custom_stopwords = self.load_stopwords_from_files(stopword_files)

        genre_matrix_path = os.path.join(self.script_dir, "genre_similarity_matrix_content.json")
        with open(genre_matrix_path, "r", encoding="utf-8") as f:
            self.genre_similarity_matrix_content = json.load(f)
        
    def load_kobert_model(self):
        """KoBERT ëª¨ë¸ ë¡œë“œ"""
        print("KoBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        model_name = "monologg/kobert"
        self.kobert_model = AutoModel.from_pretrained(model_name)
        self.kobert_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.kobert_model.eval()
        print("KoBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def enhanced_text_preprocessing(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê°•í™”"""
        if not text or pd.isna(text):
            return ""
        
        text = str(text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ë° ìˆ«ì ì²˜ë¦¬
        text = re.sub(r'[^\w\sê°€-í£a-zA-Z0-9]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

    def extract_nouns_from_text(self, text, okt):
        """í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ"""
        if not text:
            return []
        
        # ëª…ì‚¬ ì¶”ì¶œ
        nouns = okt.nouns(text)
        
        # ë‹¨ì–´ ê¸¸ì´ê°€ 1 ì´í•˜ì¸ ê²ƒ ì œê±°
        filtered_nouns = [noun for noun in nouns if len(noun) > 1]
        
        return filtered_nouns
    
    def load_stopwords_from_files(self, stopword_files=None):
        """ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶ˆìš©ì–´ íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        combined_stopwords = set()

        if stopword_files is None:
            stopwords_dir = os.path.join(self.script_dir, "stopwords")  # stopwords ë””ë ‰í† ë¦¬ ê²½ë¡œ

            try:
                stopword_files = [
                    os.path.join(stopwords_dir, fname)
                    for fname in os.listdir(stopwords_dir)
                    if fname.endswith(".txt")
                ]
            except FileNotFoundError:
                print(f"âŒ stopwords ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {stopwords_dir}")
                stopword_files = []

        for file_path in stopword_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        words = [line.strip() for line in f if line.strip()]
                        combined_stopwords.update(words)
                    print(f"âœ… ë¶ˆìš©ì–´ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {file_path} ({len(words)}ê°œ)")
                except Exception as e:
                    print(f"âŒ ë¶ˆìš©ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
            else:
                print(f"âš ï¸ ë¶ˆìš©ì–´ íŒŒì¼ ì—†ìŒ: {file_path}")

        if not combined_stopwords:
            print("âš ï¸ ë¶ˆìš©ì–´ íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆì–´ ê¸°ë³¸ ë¶ˆìš©ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return set(['í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ê²ƒ', 'ìˆ˜', 'ì ', 'ë§', 'ì•ˆ', 'ë•Œ', 'ë“±', 'í†µí•´'])

        return combined_stopwords



    def enhanced_text_preprocessing_token(self, text):
        if pd.isna(text) or not text:
            return ""

        text = self.enhanced_text_preprocessing(text)
        stopwords_pos = ['Josa', 'Eomi', 'Suffix', 'Punctuation'] 
        
        tokens = self.okt.pos(text, norm=True, stem=True)
        
        filtered_tokens = []
        for word, pos in tokens:
            if pos not in stopwords_pos:
                # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë¡œë“œí•œ custom_stopwords ì‚¬ìš©
                if word not in self.custom_stopwords: 
                    filtered_tokens.append(word)
        
        return ' '.join(filtered_tokens)
    
    def extract_keywords(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ + ë¶ˆìš©ì–´ ì œê±° + ê°€ì¤‘ì¹˜ ì ìš©)"""
        if not text or pd.isna(text):
            return []
        
        # OKT í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ë©´ ë” íš¨ìœ¨ì )
        if not hasattr(self, 'okt'):
            self.okt = Okt()
        
        # í•œêµ­ì–´ ì˜í™” ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´ (ê°€ì¤‘ì¹˜ í¬í•¨)
        movie_keywords = {
            # ì¥ë¥´ ê´€ë ¨ (ê°€ì¤‘ì¹˜ 2)
            'action': {
                'weight': 2,
                'keywords': [
                    'ì•¡ì…˜', 'ì „íˆ¬', 'ì‹¸ì›€', 'ì „ìŸ', 'ê²©íˆ¬', 'ì¶”ê²©', 'í­ë°œ', 'ì´ê²©',
                    'ë¬´ìˆ ', 'ì¹´ì²´ì´ì‹±', 'ê±´íŒŒì´íŠ¸', 'ë°°í‹€', 'ì•¡ì…˜ìŠ¤ë¦´ëŸ¬', 'ëŠì™€ë¥´',
                    'ìŠ¤íŒŒì´', 'ì²©ë³´', 'ì ì…', 'ë¯¸ì…˜', 'ì‘ì „', 'ë³µìˆ˜', 'ì •ì˜', 'ìœ„ì¥'
                ]
            },
            'romance': {
                'weight': 2,
                'keywords': [
                    'ì‚¬ë‘', 'ë¡œë§¨ìŠ¤', 'ì—°ì¸', 'ê²°í˜¼', 'ì—°ì• ', 'ì²«ì‚¬ë‘', 'ì´ë³„', 'ë§Œë‚¨',
                    'ë©œë¡œ', 'ëŸ¬ë¸ŒìŠ¤í† ë¦¬', 'ìš´ëª…', 'ì¬íšŒ', 'í”„ë¡œí¬ì¦ˆ', 'ì›¨ë”©', 'ë°ì´íŠ¸',
                    'ì¸', 'ê³ ë°±', 'ì§ì‚¬ë‘', 'ì›ê±°ë¦¬', 'êµ­ì œì—°ì• ', 'ë‚˜ì´ì°¨', 'ì‚¬ë‚´ì—°ì• '
                ]
            },
            'comedy': {
                'weight': 2,
                'keywords': [
                    'ì½”ë¯¸ë””', 'ì›ƒìŒ', 'ìœ ë¨¸', 'ì¬ë¯¸', 'ê°œê·¸', 'ìœ ì¾Œ', 'ë†ë‹´',
                    'ê°œê·¸ë§¨', 'ìƒí™©ê·¹', 'ìŠ¬ë©ìŠ¤í‹±', 'ë¡œë§¨í‹±ì½”ë¯¸ë””', 'íŒ¨ë°€ë¦¬ì½”ë¯¸ë””',
                    'ë¸”ë™ì½”ë¯¸ë””', 'í’ì', 'í•´í•™', 'ìµì‚´', 'ì½”ë¯¹', 'ìœ ë¨¸ëŸ¬ìŠ¤'
                ]
            },
            'horror': {
                'weight': 2,
                'keywords': [
                    'ê³µí¬', 'í˜¸ëŸ¬', 'ë¬´ì„œìš´', 'ê·€ì‹ ', 'ì¢€ë¹„', 'ê´´ë¬¼', 'ì•…ë ¹',
                    'ì‚¬ì´ì½”', 'ì‚´ì¸ë§ˆ', 'ì—°ì‡„ì‚´ì¸', 'ì´ˆìì—°', 'ì˜¤ì»¬íŠ¸', 'ì—‘ì†Œì‹œì¦˜',
                    'ì €ì£¼', 'ì›í˜¼', 'ê·€ì‹ ', 'ìœ ë ¹', 'ë¬´ë¤', 'íê°€', 'ì‹¬ë ¹', 'ìœ„í—˜êµ¬ì—­'
                ]
            },
            'thriller': {
                'weight': 2,
                'keywords': [
                    'ìŠ¤ë¦´ëŸ¬', 'ê¸´ì¥', 'ì¶”ì ', 'ìˆ˜ì‚¬', 'ë²”ì£„', 'ì‚´ì¸', 'ë¯¸ìŠ¤í„°ë¦¬',
                    'ì„œìŠ¤íœìŠ¤', 'ì¶”ë¦¬', 'íƒì •', 'í˜•ì‚¬', 'ê²€ì°°', 'ë²•ì •', 'ì¬íŒ',
                    'ë‚©ì¹˜', 'í˜‘ë°•', 'ìŒëª¨', 'ë°°ì‹ ', 'ì‚¬ê¸°', 'í•´í‚¹', 'ì²©ë³´'
                ]
            },
            'drama': {
                'weight': 2,
                'keywords': [
                    'ì¸ê°„', 'ê°ë™', 'ëˆˆë¬¼', 'ì¸ìƒ', 'ì„±ì¥'
                ]
            },
            
            # í…Œë§ˆ/ì†Œì¬ ê´€ë ¨ (ê°€ì¤‘ì¹˜ 1.5)
            'theme': {
                'weight': 1.5,
                'keywords': [
                    'ë³µìˆ˜', 'ì •ì˜', 'ìš°ì •', 'ë°°ì‹ ', 'í¬ìƒ',
                    'ì„±ì¥', 'ìì•„ì‹¤í˜„', 'ê¿ˆ', 'ë„ì „', 'ê·¹ë³µ', 'í™”í•´',
                    'ê°ˆë“±', 'ëŒ€ë¦½', 'í˜‘ë ¥', 'ê²½ìŸ', 'ì„±ê³µ', 'ì‹¤íŒ¨'
                ]
            },
            'emotion': {
                'weight': 1.5,
                'keywords': [
                    'ê°ë™', 'ëˆˆë¬¼', 'ìŠ¬í””', 'ê¸°ì¨', 'í–‰ë³µ', 'ë¶„ë…¸',
                    'ì ˆë§', 'í¬ë§', 'ì‚¬ë‘', 'ì´ë³„', 'ê·¸ë¦¬ì›€', 'í–¥ìˆ˜',
                    'ì™¸ë¡œì›€', 'ê³ ë…', 'ìš°ìš¸', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ë”ì°í•œ'
                ]
            },
            
            # ë°°ê²½/ì§ì—… ê´€ë ¨ (ê°€ì¤‘ì¹˜ 1)
            'profession': {
                'weight': 1,
                'keywords': [
                    'ì˜ì‚¬', 'ë³€í˜¸ì‚¬', 'êµì‚¬', 'ê²½ì°°', 'ì†Œë°©ê´€', 'êµ°ì¸',
                    'ê¸°ì', 'ì‘ê°€', 'í™”ê°€', 'ìŒì•…ê°€', 'ìš”ë¦¬ì‚¬', 'ì‚¬ì—…ê°€',
                    'ì •ì¹˜ì¸', 'ì—°ì˜ˆì¸', 'ìš´ë™ì„ ìˆ˜', 'ê³¼í•™ì', 'ì—”ì§€ë‹ˆì–´'
                ]
            },
            'background': {
                'weight': 1,
                'keywords': [
                    'í˜„ëŒ€', 'ë„ì‹œ', 'ì„œìš¸', 'í•™êµ', 'ëŒ€í•™', 'íšŒì‚¬', 'ì§ì¥',
                    'ë³‘ì›', 'ë²•ì›', 'ê²½ì°°ì„œ', 'ì•„íŒŒíŠ¸', 'ì¹´í˜'
                ]
            }
        }
        
        # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        preprocessed_text = self.enhanced_text_preprocessing(text)
        
        # 2. í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ëª…ì‚¬ ì¶”ì¶œ
        nouns = self.extract_nouns_from_text(preprocessed_text, self.okt)
        
        # 3. ë¶ˆìš©ì–´ ì œê±°
        filtered_nouns = [noun for noun in nouns 
                        if noun not in self.custom_stopwords]
        
        # 4. ëª…ì‚¬ ë¹ˆë„ ê³„ì‚°
        noun_counts = Counter(filtered_nouns)
        
        # 5. ì˜í™” í‚¤ì›Œë“œ ë§¤ì¹­ ë° ê°€ì¤‘ì¹˜ ì ìš©
        keyword_scores = {}
        
        for category, category_info in movie_keywords.items():
            weight = category_info['weight']
            keywords = category_info['keywords']
            
            for keyword in keywords:
                # í‚¤ì›Œë“œê°€ ì¶”ì¶œëœ ëª…ì‚¬ì— ìˆëŠ”ì§€ í™•ì¸
                if keyword in filtered_nouns:
                    score = noun_counts[keyword] * weight
                    if keyword in keyword_scores:
                        keyword_scores[keyword] += score
                    else:
                        keyword_scores[keyword] = score
        
        # 6. ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ 20ê°œ í‚¤ì›Œë“œë§Œ ë°˜í™˜ (í‚¤ì›Œë“œë§Œ, ì ìˆ˜ ì œì™¸)
        top_keywords = [keyword for keyword, score in sorted_keywords[:20]]
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ ì¤‘ìš” ëª…ì‚¬ë„ ì¼ë¶€ í¬í•¨ (ë¹ˆë„ìˆ˜ ê¸°ì¤€)
        remaining_nouns = [noun for noun in noun_counts.most_common(10) 
                        if noun[0] not in top_keywords]
        
        # ìµœì¢… í‚¤ì›Œë“œ ê²°í•©
        final_keywords = top_keywords + [noun[0] for noun in remaining_nouns[:5]]
        
        return final_keywords[:25]  # ìµœëŒ€ 25ê°œ í‚¤ì›Œë“œ ë°˜í™˜


    def get_kobert_embedding(self, text):
        """KoBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if not text or pd.isna(text):
            return np.zeros(768)  # KoBERT ì„ë² ë”© ì°¨ì›
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (KoBERT ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ê³ ë ¤)
        text = str(text)[:512]
        
        inputs = self.kobert_tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.kobert_model(**inputs)
            # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
            embedding = outputs.last_hidden_state[:, 0, :].squeeze().detach().cpu().numpy()
        
        return embedding
    
    def calculate_genre_similarity_advanced(self, target_genres, candidate_genres):
        """ê³ ê¸‰ ì¥ë¥´ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ì¥ë¥´ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•˜ê²Œ êµ¬ì„± ê°€ëŠ¥)
        if not target_genres or not candidate_genres:
            return 0.0

        target_genres = [g.lower() for g in target_genres]
        candidate_genres = [g.lower() for g in candidate_genres]

        # ì§ì ‘ ë§¤ì¹˜ ì ìˆ˜
        direct_match = len(set(target_genres) & set(candidate_genres)) / len(set(target_genres) | set(candidate_genres))
        
        # ê°„ì ‘ ë§¤ì¹˜ ì ìˆ˜
        indirect_score = 0
        for target_genre in target_genres:
            for candidate_genre in candidate_genres:
                if target_genre.lower() in self.genre_similarity_matrix_content:
                    if candidate_genre.lower() in self.genre_similarity_matrix_content[target_genre.lower()]:
                        indirect_score += self.genre_similarity_matrix_content[target_genre.lower()][candidate_genre.lower()]
        
        indirect_score = indirect_score / (len(target_genres) * len(candidate_genres)) if target_genres and candidate_genres else 0
        
        return 0.7 * direct_match + 0.3 * indirect_score
    
    def prepare_data(self, movies_df, force_recompute=False):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ"""
        print("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

         # ìºì‹œ íŒŒì¼ ê²½ë¡œ ì •ì˜
        feature_objects_path = f"{self.cache_dir}/feature_objects.pkl"
        feature_matrices_path = f"{self.cache_dir}/feature_matrices.npz"
        movie_data_path = f"{self.cache_dir}/movie_data.pkl"
        plot_embeddings_path = f"{self.cache_dir}/plot_embeddings.npy" # KoBERT ì„ë² ë”©ë„ ì—¬ê¸°ì— í¬í•¨

        # ìºì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° force_recomputeì— ë”°ë¥¸ ë¡œë“œ/ì¬ê³„ì‚° ê²°ì •
        if not force_recompute and \
           os.path.exists(feature_objects_path) and \
           os.path.exists(feature_matrices_path) and \
           os.path.exists(movie_data_path) and \
           os.path.exists(plot_embeddings_path): # KoBERT ì„ë² ë”© íŒŒì¼ë„ í™•ì¸
            
            print("âœ… ì €ì¥ëœ íŠ¹ì„± ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
            try:
                # 1. Feature objects (Vectorizer, SVD, MLB) ë¡œë“œ
                with open(feature_objects_path, 'rb') as f:
                    feature_objects = pickle.load(f)
                    self.tfidf_title = feature_objects['tfidf_title']
                    self.tfidf_overview = feature_objects['tfidf_overview']
                    self.tfidf_keywords = feature_objects['tfidf_keywords']
                    self.svd = feature_objects['svd']
                    self.mlb_genre = feature_objects['mlb_genre']

                # 2. Feature matrices (NumPy arrays) ë¡œë“œ
                loaded_matrices = np.load(feature_matrices_path, allow_pickle=True)
                self.title_matrix = loaded_matrices['title_matrix']
                self.overview_matrix = loaded_matrices['overview_matrix']
                self.keyword_matrix = loaded_matrices['keyword_matrix']
                self.genre_matrix = loaded_matrices['genre_matrix']

                # 3. KoBERT ì„ë² ë”© ë¡œë“œ (ë³„ë„ íŒŒì¼ë¡œ ê´€ë¦¬)
                self.plot_embeddings = np.load(plot_embeddings_path)

                # 4. ì˜í™” ë°ì´í„° ë¡œë“œ
                self.movie_data = pd.read_pickle(movie_data_path)

                print("âœ… ëª¨ë“  íŠ¹ì„± ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")
                return

            except Exception as e:
                print(f"âŒ ì €ì¥ëœ íŠ¹ì„± ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë°ì´í„°ë¥¼ ìƒˆë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ì‹œ ê³„ì‚°í•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
                force_recompute = True # ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ê³„ì‚°ì„ ìœ ë„

        # ìºì‹œê°€ ì—†ê±°ë‚˜, ê°•ì œë¡œ ì¬ê³„ì‚°í•´ì•¼ í•˜ëŠ” ê²½ìš°
        print("ğŸ›  ë°ì´í„°ë¥¼ ìƒˆë¡œ ì „ì²˜ë¦¬í•˜ê³  íŠ¹ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['title', 'overview', 'genres']
        for col in required_columns:
            if col not in movies_df.columns:
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ '{col}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        movies_df['title'] = movies_df['title'].fillna('')
        movies_df['overview'] = movies_df['overview'].fillna('')
        movies_df['genres'] = movies_df['genres'].fillna('')
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        print("í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        movies_df['keywords'] = movies_df['overview'].apply(self.extract_keywords)
        movies_df['keywords_text'] = movies_df['keywords'].apply(lambda x: ' '.join(x) if x else '')
        
        movies_df['title_text'] = movies_df['title'].apply(self.enhanced_text_preprocessing_token)
        movies_df['overview_text'] = movies_df['overview'].apply(self.enhanced_text_preprocessing_token)

        self.movie_data = movies_df.copy()
        
        # 1. ì œëª© TF-IDF ë²¡í„°í™”
        print("ì œëª© TF-IDF ë²¡í„°í™” ì¤‘...")
        self.tfidf_title = TfidfVectorizer(
            max_features=5000,
            stop_words=None,
            ngram_range=(1, 2),
            min_df=1
        )
        self.title_matrix = self.tfidf_title.fit_transform(movies_df['title_text'])

        # 2. ì¤„ê±°ë¦¬ TF-IDF ë²¡í„°í™” (í˜•íƒœì†Œ ë¶„ë¦¬ ì ìš©)    
        self.tfidf_overview = TfidfVectorizer(
            max_features=10000, 
            stop_words=None, 
            ngram_range=(1, 2), 
            min_df=1 
        )
        overview_tfidf_matrix = self.tfidf_overview.fit_transform(movies_df['overview_text'])
        print("ì¤„ê±°ë¦¬ TF-IDF ë²¡í„°í™” ì™„ë£Œ.")

        # LSA (Truncated SVD)
        self.svd = TruncatedSVD(n_components=100, random_state=42)
        self.overview_matrix = self.svd.fit_transform(overview_tfidf_matrix)
        
        # 3. í‚¤ì›Œë“œ TF-IDF ë²¡í„°í™”
        print("í‚¤ì›Œë“œ TF-IDF ë²¡í„°í™” ì¤‘...")
        self.tfidf_keywords = TfidfVectorizer(
            max_features=1000,
            stop_words=None,
            ngram_range=(1, 1),
            min_df=3,  # ìµœì†Œ 2ê°œ ì˜í™”ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            max_df=0.4,  # 40% ì´ìƒ ì˜í™”ì— ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œëŠ” ì œì™¸
        )
        self.keyword_matrix = self.tfidf_keywords.fit_transform(movies_df['keywords_text'])
        
        # 4. ì¥ë¥´ ì²˜ë¦¬
        print("ì¥ë¥´ ì²˜ë¦¬ ì¤‘...")
        genre_lists = []
        for genres in movies_df['genres']:
            if isinstance(genres, str) and genres:
                genre_list = [g.strip() for g in genres.split(',')]
                genre_lists.append(genre_list)
            elif isinstance(genres, list):
                genre_lists.append(genres)
            else:
                genre_lists.append([])
        
        self.mlb_genre = MultiLabelBinarizer()
        self.genre_matrix = self.mlb_genre.fit_transform(genre_lists)

        # 5. KoBERT ì„ë² ë”©
        print("ğŸ›  ì¤„ê±°ë¦¬ ì„ë² ë”©ì„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        if self.kobert_model is None:
            self.load_kobert_model()
        
        plot_embeddings = []
        for i, overview in enumerate(movies_df['overview_text']):
            if i % 50 == 0:
                print(f"ì„ë² ë”© ì§„í–‰ë¥ : {i}/{len(movies_df)}")
            embedding = self.get_kobert_embedding(overview)
            plot_embeddings.append(embedding)

        self.plot_embeddings = np.array(plot_embeddings)
        print("ì¤„ê±°ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ.")

        self._save_all_features()
        print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")

    def _save_all_features(self):
        """ëª¨ë“  íŠ¹ì„±ì„ í•œ ë²ˆì— ì €ì¥"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        
        print("ğŸ’¾ ëª¨ë“  íŠ¹ì„± ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # 1. TF-IDF ë²¡í„°í™” ê°ì²´ë“¤ ì €ì¥
        feature_objects = {
            'tfidf_title': self.tfidf_title,
            'tfidf_overview': self.tfidf_overview,
            'tfidf_keywords': self.tfidf_keywords,
            'svd': self.svd,
            'mlb_genre': self.mlb_genre
        }
        
        # Ensure file exists before attempting to open, or handle errors
        try:
            with open(f"{self.cache_dir}/feature_objects.pkl", 'wb') as f:
                pickle.dump(feature_objects, f)
        except Exception as e:
            print(f"Error saving feature objects: {e}")
            # Optionally, re-raise or handle more gracefully

        # 2. íŠ¹ì„± í–‰ë ¬ë“¤ ì €ì¥
        # Scipy sparse matrix (CSR) toarray() to save as dense numpy array
        # Check if matrices are sparse before converting
        title_matrix_array = self.title_matrix.toarray() if hasattr(self.title_matrix, 'toarray') else self.title_matrix
        keyword_matrix_array = self.keyword_matrix.toarray() if hasattr(self.keyword_matrix, 'toarray') else self.keyword_matrix
        overview_matrix_array = self.overview_matrix if isinstance(self.overview_matrix, np.ndarray) else self.overview_matrix.toarray()


        np.savez_compressed(f"{self.cache_dir}/feature_matrices.npz",
                            title_matrix=title_matrix_array,
                            overview_matrix=overview_matrix_array,
                            keyword_matrix=keyword_matrix_array,
                            genre_matrix=self.genre_matrix)
        
        # KoBERT ì„ë² ë”©ì€ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        np.save(f"{self.cache_dir}/plot_embeddings.npy", self.plot_embeddings)

        # 3. ì˜í™” ë°ì´í„° ì €ì¥ (ì „ì²˜ë¦¬ëœ ì»¬ëŸ¼ í¬í•¨)
        self.movie_data.to_pickle(f"{self.cache_dir}/movie_data.pkl")
        
        print(f"âœ… ëª¨ë“  ë°ì´í„°ê°€ {self.cache_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def calculate_similarity_comprehensive(self, movie_idx, weights):
        """í¬ê´„ì ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        # 1. KoBERT ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ (ì˜ë¯¸ì  ìœ ì‚¬ë„)
        target_plot_embedding = self.plot_embeddings[movie_idx].reshape(1, -1)
        kobert_similarities = cosine_similarity(target_plot_embedding, self.plot_embeddings)[0]
        kobert_similarities[kobert_similarities < 0] = 0
        
        # 2. ì¤„ê±°ë¦¬ TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ (ì–´íœ˜ì  ìœ ì‚¬ë„)
        target_overview_vector = self.overview_matrix[movie_idx]
        overview_similarities = cosine_similarity(target_overview_vector.reshape(1, -1), self.overview_matrix)[0]
        overview_similarities[overview_similarities < 0] = 0
        
        # 3. ì œëª© ìœ ì‚¬ë„
        target_title_vector = self.title_matrix[movie_idx]
        title_similarities = cosine_similarity(target_title_vector.reshape(1, -1), self.title_matrix)[0]
        
        # 4. í‚¤ì›Œë“œ ìœ ì‚¬ë„
        target_keyword_vector = self.keyword_matrix[movie_idx]
        keyword_similarities = cosine_similarity(target_keyword_vector.reshape(1, -1), self.keyword_matrix)[0]
        
        # 5. ì¥ë¥´ ìœ ì‚¬ë„
        target_genres = self.movie_data.iloc[movie_idx]['genres'].split(',') if self.movie_data.iloc[movie_idx]['genres'] else []
        target_genres = [g.strip() for g in target_genres]
        
        genre_similarities = []
        for i in range(len(self.movie_data)):
            candidate_genres = self.movie_data.iloc[i]['genres'].split(',') if self.movie_data.iloc[i]['genres'] else []
            candidate_genres = [g.strip() for g in candidate_genres]
            
            similarity = self.calculate_genre_similarity_advanced(target_genres, candidate_genres)
            genre_similarities.append(similarity)
        
        genre_similarities = np.array(genre_similarities)
        
        # 6. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
        final_similarities = (
            weights['kobert'] * kobert_similarities +
            weights['overview_tfidf'] * overview_similarities +
            weights['title'] * title_similarities +
            weights['keywords'] * keyword_similarities +
            weights['genre'] * genre_similarities
        )
        
        return (final_similarities, kobert_similarities, overview_similarities, 
                title_similarities, keyword_similarities, genre_similarities)
    
    def adaptive_weights(self, movie_idx):
        """ì˜í™” íŠ¹ì„±ì— ë”°ë¥¸ ì ì‘ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        movie_info = self.movie_data.iloc[movie_idx]
        
        # 1. ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì • (ì´í•© 100%)
        weights = {
            'kobert': 0.20,
            'overview_tfidf': 0.35,
            'title': 0.10,  # ì œëª© ìœ ì‚¬ë„ ë¬¸ì œ í•´ê²° ì „ê¹Œì§€ëŠ” ë‚®ê²Œ ìœ ì§€
            'keywords': 0.20, # í‚¤ì›Œë“œ ì¤‘ìš”ì„±ì„ ë†’ì„
            'genre': 0.25   # ì¥ë¥´ ì¤‘ìš”ì„±ì„ ë†’ì„
        }

        # ëª¨ë“  ê°€ì¤‘ì¹˜ ì¡°ì •ì€ ìƒëŒ€ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë¯€ë¡œ, ì´í•©ì´ 1ì´ ë˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ì ˆëŒ€ê°’ìœ¼ë¡œ ë”í•˜ê³  ë¹¼ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¹„ìœ¨ë¡œ ì¡°ì •í•˜ê±°ë‚˜, ë§ˆì§€ë§‰ì— ì •ê·œí™”í•´ì•¼ í•©ë‹ˆë‹¤.


        # 2. ì¤„ê±°ë¦¬ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        overview_length = len(movie_info['overview']) if movie_info['overview'] else 0

        if overview_length > 300:  # ê¸´ ì¤„ê±°ë¦¬: ì¤„ê±°ë¦¬ ê´€ë ¨ ê°€ì¤‘ì¹˜ ê°•í™”
            weights['kobert'] += 0.03
            weights['overview_tfidf'] += 0.05
            weights['genre'] -= 0.04 # ìƒëŒ€ì ìœ¼ë¡œ ì¥ë¥´ ì¤‘ìš”ë„ ê°ì†Œ
            weights['keywords'] -= 0.04
        elif overview_length < 50:  # ì§§ì€ ì¤„ê±°ë¦¬: ì œëª©/ì¥ë¥´/í‚¤ì›Œë“œ ê°•í™”, ì¤„ê±°ë¦¬ ê´€ë ¨ ì•½í™”
            weights['title'] += 0.05 # ì œëª© ìœ ì‚¬ë„ ê°œì„  ì‹œ íš¨ê³¼ì 
            weights['genre'] += 0.05
            weights['keywords'] += 0.05
            weights['kobert'] -= 0.07 # KoBERT ë‚®ì¶¤
            weights['overview_tfidf'] -= 0.08 # TF-IDF ë‚®ì¶¤

        # 3. ì¥ë¥´ ìˆ˜ì— ë”°ë¥¸ ì¡°ì • (if-elif-else êµ¬ì¡° ì‚¬ìš©)
        genre_count = len(movie_info['genres'].split(',')) if movie_info['genres'] else 0

        if genre_count == 1:  # ë‹¨ì¼ ì¥ë¥´: í•´ë‹¹ ì¥ë¥´ì˜ íŠ¹ì„±ì„ ê°•í•˜ê²Œ ë°˜ì˜
            weights['genre'] += 0.10
            weights['overview_tfidf'] -= 0.05
            weights['keywords'] -= 0.05
            weights['kobert'] -= 0.02
        elif genre_count > 3:  # ë§ì€ ì¥ë¥´: íŠ¹ì • ì¥ë¥´ì— ëœ ì¹˜ìš°ì¹˜ë„ë¡
            weights['genre'] -= 0.03
            weights['kobert'] += 0.03 # KoBERT ì˜í–¥ë ¥ì„ ë‹¤ì‹œ ë†’ì—¬ ì „ì²´ ì¤„ê±°ë¦¬ ë¬¸ë§¥ ë°˜ì˜
        # else: (2ê°œì˜ ì¥ë¥´ì¸ ê²½ìš° ê¸°ë³¸ ê°€ì¤‘ì¹˜ ìœ ì§€)

        # 4. íŠ¹ì • ì¥ë¥´ì— ëŒ€í•œ ì¡°ì • (ê°€ì¥ êµ¬ì²´ì ì¸ ì¡°ê±´ë¶€í„° ë°°ì¹˜)
        genres = movie_info['genres'].lower() if movie_info['genres'] else ''

        # ìŠ¤ë¦´ëŸ¬ì™€ ë“œë¼ë§ˆ ì¡°í•© (ê¸°ìƒì¶©, í•˜ë…€ ë“±)
        if 'ë“œë¼ë§ˆ' in genres and 'ìŠ¤ë¦´ëŸ¬' in genres and 'ì½”ë¯¸ë””' in genres: # ê¸°ìƒì¶© ê°™ì€ ê²½ìš°
            weights['genre'] += 0.15 # ì¥ë¥´ ë§¤ìš° ì¤‘ìš”
            weights['keywords'] += 0.05
            weights['kobert'] -= 0.05 # KoBERT ë¹„ì¤‘ ê°ì†Œ
        elif 'ë“œë¼ë§ˆ' in genres and 'ìŠ¤ë¦´ëŸ¬' in genres: # ì¼ë°˜ì ì¸ ë“œë¼ë§ˆ/ìŠ¤ë¦´ëŸ¬
            weights['genre'] += 0.10
            weights['keywords'] += 0.05
            weights['overview_tfidf'] += 0.02 # ì¤„ê±°ë¦¬ í•µì‹¬ ë‹¨ì–´ ì¤‘ìš”ë„ë„ ë†’ì„
        elif 'sf' in genres or 'ê³µí¬' in genres: # 'ê´´ë¬¼' ê°™ì€ ê²½ìš°
            weights['genre'] += 0.15 # ì¥ë¥´ ë§¤ìš° ì¤‘ìš”
            weights['keywords'] += 0.10 # SF/ê³µí¬ëŠ” í‚¤ì›Œë“œê°€ ì¤‘ìš” (ê´´ë¬¼, ì¢€ë¹„ ë“±)
            weights['overview_tfidf'] += 0.05 # ì¤„ê±°ë¦¬ ë‚´ í•µì‹¬ ë‹¨ì–´ ì¤‘ìš”
            weights['kobert'] -= 0.10 # KoBERT ë¹„ì¤‘ ëŒ€í­ ê°ì†Œ
        elif 'ì•¡ì…˜' in genres:
            weights['keywords'] += 0.05
            weights['overview_tfidf'] += 0.05 # ì•¡ì…˜ì€ ì¤„ê±°ë¦¬ ë‚´ ë™ì‘ ë¬˜ì‚¬ê°€ ì¤‘ìš”
            weights['title'] -= 0.05 # ì œëª©ë³´ë‹¤ëŠ” ë‚´ìš©ì´ ì¤‘ìš”
        elif 'ë¡œë§¨ìŠ¤' in genres:
            weights['kobert'] += 0.03 # ê°ì„±ì ì¸ ì¤„ê±°ë¦¬ ìœ ì‚¬ì„± ì¤‘ìš”
            weights['overview_tfidf'] += 0.03
            weights['keywords'] -= 0.03
            weights['genre'] -= 0.03 # ë¡œë§¨ìŠ¤ëŠ” ì¥ë¥´ê°€ ë„“ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
        elif 'ì½”ë¯¸ë””' in genres:
            weights['kobert'] += 0.02 # ì½”ë¯¸ë””ë„ ì¤„ê±°ë¦¬ì˜ ë‰˜ì•™ìŠ¤ê°€ ì¤‘ìš”
            weights['genre'] += 0.05 # ì½”ë¯¸ë”” ì¥ë¥´ì˜ íŠ¹ìƒ‰
            weights['keywords'] -= 0.02

        # ìŒìˆ˜ ê°€ì¤‘ì¹˜ ë°©ì§€ ë° ìµœì†Œê°’ ì„¤ì • (ì„ íƒ ì‚¬í•­)
        for key in weights:
            if weights[key] < 0.01: # ìµœì†Œ ê°€ì¤‘ì¹˜ ì„¤ì • (ì˜ˆ: 1%)
                weights[key] = 0.01

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(weights.values())
        weights = {k: v/total_weight for k, v in weights.items()}
        
        return weights

    def recommend_movies(self, movie_idx=None, top_n=10, weights=None, use_adaptive_weights=True, content_id=None, content_type=None):
       
        if movie_idx is None:
            # ë³µí•©í‚¤ë¥¼ í†µí•œ ì¸ë±ìŠ¤ íƒìƒ‰
            if content_id is not None and content_type is not None:
                matches = self.movie_data[
                    (self.movie_data['id'] == content_id) &
                    (self.movie_data['type'] == content_type)
                ]
                if matches.empty:
                    print(f"id '{content_id}'ì™€ type '{content_type}'ì— í•´ë‹¹í•˜ëŠ” ì˜í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                movie_idx = matches.index[0]
                actual_title = matches.iloc[0]['title']

            else:
                raise ValueError("movie_idx ë˜ëŠ” contentId + type ë˜ëŠ” movie_title ì¤‘ í•˜ë‚˜ëŠ” ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            actual_title = self.movie_data.iloc[movie_idx]['title']

        # ê°€ì¤‘ì¹˜ ì„¤ì •
        if use_adaptive_weights:
            weights = self.adaptive_weights(movie_idx)
            print(f"ì ì‘ì  ê°€ì¤‘ì¹˜ ì‚¬ìš©: {weights}")
        elif weights is None:
            weights = {
                'kobert': 0.15,
                'overview_tfidf': 0.35,
                'title': 0.10,
                'keywords': 0.25,
                'genre': 0.15
            }

        print(f"\nê¸°ì¤€ ì˜í™”: {actual_title}")
        print(f"ì¥ë¥´: {self.movie_data.iloc[movie_idx]['genres']}")
        print(f"ì¤„ê±°ë¦¬: {self.movie_data.iloc[movie_idx]['overview'][:400]}...")
        print(f"   í‚¤ì›Œë“œ: {self.movie_data.iloc[movie_idx]['keywords']}")

        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self.calculate_similarity_comprehensive(movie_idx, weights)
        final_similarities = similarities[0]

        # ìê¸° ìì‹  ì œì™¸ ìƒìœ„ Nê°œ
        similar_indices = np.argsort(final_similarities)[::-1]
        similar_indices = [idx for idx in similar_indices if idx != movie_idx][:top_n]

        # ì¶”ì²œ ê²°ê³¼
        recommendations = []
        for idx in similar_indices:
            movie_info = {
                'content_id': self.movie_data.iloc[idx].get('id'),
                'content_type': self.movie_data.iloc[idx].get('type'),
                'title': self.movie_data.iloc[idx]['title'],
                'genres': self.movie_data.iloc[idx]['genres'],
                'overview': self.movie_data.iloc[idx]['overview'],
                'keywords': self.movie_data.iloc[idx]['keywords'],
                'total_similarity': final_similarities[idx],
                'poster_path': self.movie_data.iloc[idx]['poster_path'],
            }
            recommendations.append(movie_info)

        return recommendations

    
    def display_recommendations(self, recommendations):
        """ì¶”ì²œ ê²°ê³¼ ì¶œë ¥"""
        if not recommendations:
            print("ì¶”ì²œí•  ì˜í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\n=== ì¶”ì²œ ì˜í™” TOP {len(recommendations)} ===")
        
        for i, movie in enumerate(recommendations, 1):
            print(f"\n{i}. {movie['title']}")
            print(f"content_id: {movie['content_id']}, content_type: {movie['content_type']}")
            print(f"   ì´ ìœ ì‚¬ë„: {movie['total_similarity']:.3f}")
            print(f"   ì½˜í…ì¸  í¬ìŠ¤í„°: {movie['poster_path']}")

if __name__ == "__main__":
    # ë°ì´í„°ì…‹ ë¡œë“œ (ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    movies_df = pd.read_csv('content_data.csv', encoding='utf-8-sig')
    print(f"âœ… 'content_data.csv' íŒŒì¼ ë¡œë“œ ì„±ê³µ. ì´ {len(movies_df)}ê°œ ë°ì´í„°.")
    
    # ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    recommender = ImprovedMovieRecommendationSystem(cache_dir_name="./cached_features")
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    recommender.prepare_data(movies_df)
    
    # ì ì‘ì  ê°€ì¤‘ì¹˜ë¡œ ì¶”ì²œ ì‹¤í–‰
    recommendations1 = recommender.recommend_movies(
        content_id= 5707,
        content_type= "tv",
        top_n=8,
        use_adaptive_weights=True
    )
    recommender.display_recommendations(recommendations1)