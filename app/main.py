import os
import sys
import io
from dotenv import load_dotenv
load_dotenv()
# main.py
import boto3
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.orm import Session
from sqlalchemy import text
from pydantic import BaseModel
from pathlib import Path
from datetime import datetime
from typing import List
import pandas as pd
import numpy as np
import json
import traceback
from contextlib import asynccontextmanager
import pandas as pd
import logging
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìœ„í•´ ì¶”ê°€

from config import settings
from database import check_db_connection, get_db
from popcorithm.models import (
    MovieRecommendation, 
    RecommendationResponse
)

from popcorithm.create_popcorithm_csv import create_movie_metadata_csv
from popcorithm.get_user_pattern import get_user_recent_activities
from popcorithm.popcorithm import calculate_user_preferences
from popcorithm.calc_cosine import calculate_recommendations, load_movie_metadata_with_vectors
from content_based_recommender.contents_recommender_py import ImprovedMovieRecommendationSystem
from content_based_recommender.schemas import RecommendRequest, ContentRecommendationListResponse, RecommendationListResponse
from content_based_recommender.data_saver import get_existing_recommendations, save_recommendations_to_db, get_top_ranked_content, build_recommendation_responses, check_user_exists

# from .popcorithm.create_popcorithm_csv import create_movie_metadata_csv, create_metadata_only_csv
# from .popcorithm.get_user_pattern import get_user_recent_activities
# from .popcorithm.popcorithm import calculate_user_preferences
# from .popcorithm.calc_cosine import calculate_recommendations, load_movie_metadata_with_vectors

from persona_based_recommender.state import pbr_app_state
from persona_based_recommender.persona_router import persona_recommender_router
from persona_based_recommender.data_loader import load_all_data

from filtering.filtering_router import filtering_recommender_router
from filtering.data_loader import load_all_filtering_data

APP_ROOT_DIR = Path(__file__).parent # '/app'
S3_BUCKET_NAME = settings.MY_AWS_S3_BUCKET_NAME
s3 = boto3.client(
    's3',
    aws_access_key_id=settings.MY_AWS_ACCESS_KEY_ID,
    aws_secret_access_key=settings.MY_AWS_SECRET_ACCESS_KEY,
    region_name=settings.MY_AWS_S3_REGION
)

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

cached_movie_df = None
cached_features = None
cached_movie_vectors = None

def load_csv_from_s3(bucket_name: str, key: str, chunksize: int = 10000) -> pd.DataFrame:
    """S3ì—ì„œ CSV íŒŒì¼ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì½ì–´ì„œ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    try:
        logger.info(f"S3ì—ì„œ CSV ë¡œë“œ ì‹œì‘: s3://{bucket_name}/{key} (ì²­í¬ í¬ê¸°: {chunksize})")
        
        # S3ì—ì„œ íŒŒì¼ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
        response = s3.get_object(Bucket=bucket_name, Key=key)
        
        # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ BytesIOë¡œ ë³€í™˜
        csv_data = io.BytesIO(response['Body'].read())
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ì–´ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬
        chunk_list = []
        total_rows = 0
        
        logger.info("ì²­í¬ ë‹¨ìœ„ë¡œ CSV ë°ì´í„° ì½ê¸° ì‹œì‘...")
        
        # pandasì˜ chunksize íŒŒë¼ë¯¸í„° ì‚¬ìš©
        for chunk_num, chunk in enumerate(pd.read_csv(csv_data, chunksize=chunksize), 1):
            chunk_list.append(chunk)
            total_rows += len(chunk)
            
            if chunk_num % 10 == 0:  # 10ì²­í¬ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
                logger.info(f"ì²­í¬ {chunk_num} ì²˜ë¦¬ ì™„ë£Œ, ëˆ„ì  í–‰ ìˆ˜: {total_rows}")
                
            # ë©”ëª¨ë¦¬ ê´€ë¦¬: ì¤‘ê°„ì— ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            if chunk_num % 50 == 0:
                gc.collect()
        
        logger.info(f"ëª¨ë“  ì²­í¬ ì½ê¸° ì™„ë£Œ. DataFrame í•©ì¹˜ê¸° ì‹œì‘...")
        
        # ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ê¸°
        df = pd.concat(chunk_list, ignore_index=True)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del chunk_list
        gc.collect()
        
        logger.info(f"S3 CSV ë¡œë“œ ì„±ê³µ: {len(df)}ê°œ í–‰, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ì™„ë£Œ")
        return df
        
    except Exception as e:
        logger.error(f"S3 CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        raise HTTPException(status_code=500, detail=f"S3ì—ì„œ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")

def load_json_from_s3(bucket_name: str, key: str, check_size: bool = True) -> dict:
    """S3ì—ì„œ JSON íŒŒì¼ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì½ì–´ì„œ dictë¡œ ë°˜í™˜"""
    try:
        logger.info(f"S3ì—ì„œ JSON ë¡œë“œ ì‹œì‘: s3://{bucket_name}/{key}")
        
        # íŒŒì¼ í¬ê¸° ë¨¼ì € í™•ì¸ (ì˜µì…˜)
        if check_size:
            head_response = s3.head_object(Bucket=bucket_name, Key=key)
            file_size_mb = head_response['ContentLength'] / (1024 * 1024)
            logger.info(f"JSON íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
            
            # í° íŒŒì¼ì˜ ê²½ìš° ê²½ê³  ë¡œê·¸
            if file_size_mb > 100:  # 100MB ì´ìƒ
                logger.warning(f"í° JSON íŒŒì¼ ê°ì§€ ({file_size_mb:.2f} MB). ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.")
        
        # S3ì—ì„œ íŒŒì¼ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
        response = s3.get_object(Bucket=bucket_name, Key=key)
        
        # JSON ë°ì´í„° íŒŒì‹±
        json_content = response['Body'].read().decode('utf-8')
        json_data = json.loads(json_content)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del json_content
        gc.collect()
        
        logger.info(f"S3 JSON ë¡œë“œ ì„±ê³µ")
        return json_data
        
    except Exception as e:
        logger.error(f"S3 JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        raise HTTPException(status_code=500, detail=f"S3ì—ì„œ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
    
async def initialize_local_recommender_system():
    global recommender_system, movies_dataframe

    logger.info("ğŸ”§ ë¡œì»¬ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")

    try:
        # Docker í™˜ê²½ì—ì„œëŠ” /appì´ ì‘ì—… ë””ë ‰í† ë¦¬
        current_file_dir = Path(__file__).resolve().parent  # /app
        
        # data_processing ë””ë ‰í† ë¦¬ê°€ /appê³¼ ê°™ì€ ë ˆë²¨ì— ìˆë‹¤ë©´
        data_file_path = current_file_dir / 'content_based_recommender' / 'content_data.csv'  # /app/data_processing/content_data.csv
        
        # ë˜ëŠ” ìƒìœ„ ë””ë ‰í† ë¦¬ì— ìˆë‹¤ë©´ ì´ë ‡ê²Œ:
        # project_root = current_file_dir.parent  # Dockerì—ì„œëŠ” í•„ìš” ì—†ì„ ìˆ˜ ìˆìŒ
        # data_file_path = project_root / 'data_processing' / 'content_data.csv'
        
        abs_data_file_path = str(data_file_path)

        logger.info(f"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹œë„: {abs_data_file_path}")

        movies_dataframe = pd.read_csv(abs_data_file_path, encoding='utf-8-sig', header=0)

        recommender_system = ImprovedMovieRecommendationSystem(cache_dir_name="cached_features")
        recommender_system.prepare_data(movies_dataframe)

        logger.info("âœ… ë¡œì»¬ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.")

    except Exception as e:
        traceback.print_exc()
        logger.error(f"âŒ ë¡œì»¬ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise RuntimeError(f"ë¡œì»¬ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

def load_vectors_from_s3(bucket_name: str, key: str) -> np.ndarray:
    """S3ì—ì„œ NPY íŒŒì¼ì„ ë¹ ë¥´ê²Œ ë¡œë“œ"""
    try:
        logger.info(f"S3ì—ì„œ NPY ë¡œë“œ: s3://{bucket_name}/{key}")
        
        response = s3.get_object(Bucket=bucket_name, Key=key)
        npy_data = io.BytesIO(response['Body'].read())
        vectors = np.load(npy_data)
        
        logger.info(f"NPY ë¡œë“œ ì™„ë£Œ: {vectors.shape}, {vectors.nbytes / 1024**2:.2f} MB")
        return vectors
        
    except Exception as e:
        logger.error(f"NPY ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"NPY ë¡œë“œ ì‹¤íŒ¨: {e}")
    
def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”"""
    logger.info("DataFrame ë©”ëª¨ë¦¬ ìµœì í™” ì‹œì‘...")
    
    initial_memory = df.memory_usage(deep=True).sum() / 1024**2
    logger.info(f"ìµœì í™” ì „ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.2f} MB")
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ ìµœì í™”
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')
    
    # ë¬¸ìì—´ ì»¬ëŸ¼ì„ categoryë¡œ ë³€í™˜ (ì¤‘ë³µì´ ë§ì€ ê²½ìš°)
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].nunique() / len(df) < 0.5:  # ìœ ë‹ˆí¬ ê°’ì´ 50% ë¯¸ë§Œì¸ ê²½ìš°
            df[col] = df[col].astype('category')
    
    final_memory = df.memory_usage(deep=True).sum() / 1024**2
    logger.info(f"ìµœì í™” í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} MB")
    logger.info(f"ë©”ëª¨ë¦¬ ì ˆì•½: {initial_memory - final_memory:.2f} MB ({(initial_memory - final_memory) / initial_memory * 100:.1f}%)")
    
    return df
    
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ë©”ì¸ FastAPI ì• í”Œ ì‹œì‘: ëª¨ë“  ì¶”ì²œ ì‹œìŠ¤í…œ ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘.")

    """ì„œë²„ ì‹œì‘ ì‹œ ë¶ˆë³€ ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ"""
    global cached_movie_df, cached_features, cached_movie_vectors
    
    print("ğŸš€ ì˜í™” ë°ì´í„° ìºì‹± ì‹œì‘...")
    
    try:
        # # S3ì—ì„œ CSV ë¡œë“œ (ì²­í¬ í¬ê¸° ì¡°ì • ê°€ëŠ¥)
        # csv_key = 'popcorithm_contents_metadata.csv'
        # print("ğŸ“Š CSV íŒŒì¼ ë¡œë”© ì¤‘...")
        # cached_movie_df = load_csv_from_s3(S3_BUCKET_NAME, csv_key, chunksize=5000)  # ì²­í¬ í¬ê¸°ë¥¼ 5000ìœ¼ë¡œ ì„¤ì •
        
        # # DataFrame ë©”ëª¨ë¦¬ ìµœì í™”
        # cached_movie_df = optimize_dataframe_memory(cached_movie_df)
        # print(f"âœ… S3 CSV ë¡œë“œ ë° ìµœì í™” ì™„ë£Œ: {len(cached_movie_df)}ê°œ ì˜í™”")
        
        # # S3ì—ì„œ JSON ë¡œë“œ
        # json_key = 'popcorithm_with_features.json'
        # print("ğŸ“‹ JSON íŒŒì¼ ë¡œë”© ì¤‘...")
        # cached_features = load_json_from_s3(S3_BUCKET_NAME, json_key, check_size=True)
        # print(f"âœ… S3 JSON ë¡œë“œ ì™„ë£Œ: {len(cached_features['actors'])}ëª… ë°°ìš°")
        
        # # ë²¡í„° ë¯¸ë¦¬ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
        # print("ğŸ”¢ ë²¡í„° ë¡œë“œ ì¤‘...")
        # cached_movie_vectors = load_vectors_from_s3(S3_BUCKET_NAME, 'movie_vectors.npy')
        
        # # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
        # gc.collect()
        
        # print("ğŸ‰ S3 ìºì‹± ì™„ë£Œ! ì¶”ì²œ API ì¤€ë¹„ë¨")
        
        # # data_loaderì˜ load_all_data í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ
        # load_all_data()
        load_all_filtering_data()
        await initialize_local_recommender_system()

        logger.info("ì¶”ì²œ ì‹œìŠ¤í…œ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ.")
        yield
        
    except Exception as e:
        logger.error(f"ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        raise RuntimeError("ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨") from e

# Router ë¥¼ í†µí•´ ë©”ì¸ì—ë‹¤ ì—°ê²°
# CORS ì„¤ì • #
app = FastAPI(title="Popco Recommender API", version="1.0.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://popco.site", "http://www.popco.site"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["Authorization", "Cache-Control", "Content-Type"],
)

app.include_router(persona_recommender_router)
app.include_router(filtering_recommender_router, prefix="/recommends/filters")

@app.get("/", tags=["ê¸°ë³¸"])
def root():
    return {
        "message": "Popco ì¶”ì²œ ì‹œìŠ¤í…œ API",
        "status": "running",
        "docker_env": os.getenv("DOCKER_ENV") is not None
    }

@app.get("/health", tags=["ì‹œìŠ¤í…œ"])
def health_check():
    """ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸
        db_connected = check_db_connection()
        
        return {
            "status": "healthy",
            "database": "connected" if db_connected else "disconnected"
        }
        
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.get("/debug/data-sizes", tags=["ê´€ë¦¬ì"])
def check_data_sizes():
    """ìºì‹œëœ ë°ì´í„° í¬ê¸° í™•ì¸, ë””ë²„ê¹… ìš©ë„"""
    return {
        "movie_df_shape": cached_movie_df.shape if cached_movie_df is not None else "None",
        "movie_vectors_shape": cached_movie_vectors.shape if cached_movie_vectors is not None else "None", 
        "features_actors_count": len(cached_features['actors']) if cached_features else "None",
        "features_total_size": len(cached_features['genres']) + len(cached_features['actors']) + len(cached_features['directors']) if cached_features else "None"
    }

@app.post("/admin/generate-vectors-npy", tags=["ê´€ë¦¬ì"])
def generate_vectors_npy():
    """ê´€ë¦¬ìê°€ ë²¡í„°ë¥¼ numpy ë°”ì´ë„ˆë¦¬ íŒŒì¼ë¡œ ë¯¸ë¦¬ ìƒì„±í•˜ì—¬ S3ì— ì—…ë¡œë“œ, í”„ë¡ íŠ¸ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
    try:
        print("ğŸ”¢ ë²¡í„° NPY íŒŒì¼ ìƒì„± ì‹œì‘...")
        
        # 1. CSVì—ì„œ ë²¡í„° ë°ì´í„° ë¡œë“œ
        csv_key = 'popcorithm_with_vectors.csv'
        df = load_csv_from_s3(S3_BUCKET_NAME, csv_key, chunksize=5000)
        
        # 2. ë²¡í„° ë³€í™˜ (ê¸°ì¡´ ë°©ì‹)
        print("ë²¡í„° ë³€í™˜ ì¤‘...")
        movie_vectors = np.array([
            list(map(float, row['vector'].split(',')))
            for _, row in df.iterrows()
        ], dtype=np.float32)
        
        # 3. NPY íŒŒì¼ë¡œ S3 ì—…ë¡œë“œ
        npy_buffer = io.BytesIO()
        np.save(npy_buffer, movie_vectors)
        npy_buffer.seek(0)
        
        s3.put_object(
            Bucket=S3_BUCKET_NAME,
            Key='movie_vectors.npy',
            Body=npy_buffer.getvalue()
        )
        
        print(f"âœ… NPY íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {movie_vectors.shape}")
        
        return {
            "message": "ë²¡í„° NPY íŒŒì¼ ìƒì„± ë° S3 ì—…ë¡œë“œ ì™„ë£Œ",
            "shape": movie_vectors.shape,
            "file_size_mb": movie_vectors.nbytes / 1024**2
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"NPY ìƒì„± ì‹¤íŒ¨: {e}")
    
@app.post("/admin/popcorithms/create-csv", tags=["ê´€ë¦¬ì"])
def init_popcorithm_csv():
    """ê´€ë¦¬ìê°€ íŒì½”ë¦¬ì¦˜ ë²¡í„° csv ë¥¼ ìƒì„±í•˜ëŠ” API ì…ë‹ˆë‹¤. í”„ë¡ íŠ¸ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
    try:
        create_movie_metadata_csv()
        return {"message": "ì„œë²„ì— movie_metadata.csv íŒŒì¼ ìƒì„±ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        traceback.print_exc()  # ìë°”ì˜ printStackTraceì™€ ë™ì¼
        
        # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ê°€ì ¸ì˜¤ê¸°
        error_details = traceback.format_exc()
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
@app.post("/admin/create-metadata-csv", tags=["ê´€ë¦¬ì"])
def create_metadata_csv_api():
    """ ê´€ë¦¬ìê°€ ìˆœìˆ˜ ì½˜í…ì¸  ë©”íƒ€ë°ì´í„° csv ë¥¼ ìƒì„±í•˜ëŠ” API, í”„ë¡ íŠ¸ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. """
    try:
        print("ğŸ“‹ ìˆœìˆ˜ ë©”íƒ€ë°ì´í„° CSV ìƒì„± ì‹œì‘...")
        result_df = create_metadata_only_csv()
        
        return {
            "message": "ìˆœìˆ˜ ë©”íƒ€ë°ì´í„° CSV ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "details": {
                "file_name": "popcorithm_contents_metadata.csv",
                "total_movies": len(result_df),
                "file_type": "metadata_only",
                "estimated_size": "~50MB",
                "generation_time": "ë¹ ë¦„ (30ì´ˆ ë‚´ì™¸)"
            }
        }
        
    except Exception as e:
        print(f"ë©”íƒ€ë°ì´í„° CSV ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500, 
            detail=f"ë©”íƒ€ë°ì´í„° CSV ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )
    
@app.get("/recommends/popcorithms/users/{userId}/limits/{limit}", tags=["ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜"])
def recommends_by_popcorithm(user_id: int, limit: int):
    """ì‚¬ìš©ìê°€ íŒì½”ë¦¬ì¦˜ ì¶”ì²œì„ ë°›ê¸°ìœ„í•´ í˜¸ì¶œë˜ì–´ì•¼ í•˜ëŠ” API ì…ë‹ˆë‹¤. userID ì™€ limitë¥¼ ë„£ì–´ì„œ ì‚¬ìš©í•´ì£¼ì„¸ìš”."""
    try:
        # 1ë‹¨ê³„: ì‚¬ìš©ì í™œë™ ì¡°íšŒ
        activities = get_user_recent_activities(user_id) # activites : List[Dict]
        
        # 2ë‹¨ê³„: ì„ í˜¸ë„ ê³„ì‚°
        preferences = calculate_user_preferences(activities) # preferences : Dict
        
        # 3ë‹¨ê³„: ì¶”ì²œ ê³„ì‚°  
        watched_movies = [activity['movie_id'] for activity in activities] 
        
        recommendations = calculate_recommendations(preferences, cached_movie_df, cached_features, cached_movie_vectors, watched_movies, limit) # 
        
        # 4ë‹¨ê³„: ì‘ë‹µ ë°˜í™˜
        return RecommendationResponse(
            user_id=user_id,
            recommendations=recommendations,
            total_count=len(recommendations),
            generated_at=datetime.now().isoformat()
        )
        
    except Exception as e:
                # ìƒì„¸ ì—ëŸ¬ ì •ë³´ ì¶œë ¥
        import traceback
        print("=== ì—ëŸ¬ ë°œìƒ! ===")
        print(f"ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
        print(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        print("\n=== ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ===")
        traceback.print_exc()  # ì½˜ì†”ì— ì¶œë ¥
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/recommends/contents", response_model=RecommendationListResponse, tags=["ì¶”ì²œ ì‹œìŠ¤í…œ"])
async def recommend_movies_api(request: RecommendRequest, db: Session = Depends(get_db)):
    if recommender_system is None:
        raise HTTPException(status_code=503, detail="ì¶”ì²œ ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    check_user_exists(db, request.user_id)

    try:
        result = get_top_ranked_content(db, batch_type=request.type)

        if result:
            content_id, content_type = result

            # 1. DBì—ì„œ ê¸°ì¡´ ì¶”ì²œ ë°ì´í„° í™•ì¸
            existing_recommendations = get_existing_recommendations(
                db, content_id, content_type, request.user_id
            )
            
            # 2. DBì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
            if existing_recommendations:
                return RecommendationListResponse(recommendations=existing_recommendations)
            
            # 3. DBì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¶”ì²œ ì‹œìŠ¤í…œ ì‹¤í–‰
            raw_recommendations = recommender_system.recommend_movies(
                content_id=content_id,
                content_type=content_type,
                top_n=8,
                use_adaptive_weights=True
            )

            if raw_recommendations is None:
                raise HTTPException(
                    status_code=404, 
                    detail=f"ID '{content_id}', Type '{content_type}'ì— ì¼ì¹˜í•˜ëŠ” ì˜í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            # 4. dictë¥¼ RecommendationResponseë¡œ ë³€í™˜
            recommendations = []
            for rec_dict in raw_recommendations:
                recommendation = ContentRecommendationListResponse(
                    content_id=rec_dict['content_id'],
                    content_type=rec_dict.get('content_type'),
                    title=rec_dict['title'],
                    total_similarity=rec_dict['total_similarity'],
                    poster_path=rec_dict['poster_path']
                )
                recommendations.append(recommendation)

            # 5. ì¶”ì²œ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (RecommendationResponse ê°ì²´ë“¤ë¡œ)
            save_success = save_recommendations_to_db(
                db, content_id, content_type, recommendations
            )
            
            if not save_success:
                print("DB ì €ì¥ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ì¶”ì²œ ê²°ê³¼ëŠ” ë°˜í™˜í•©ë‹ˆë‹¤.")

            # user_idê°€ ìˆë‹¤ë©´ ì „ë‹¬
            return build_recommendation_responses(db, recommendations, user_id=request.user_id)
        
        else:
            # ì¸ê¸° ì½˜í…ì¸ ê°€ ì—†ëŠ” ê²½ìš°
            raise HTTPException(status_code=404, detail="ë­í‚¹ëœ ì¸ê¸° ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
